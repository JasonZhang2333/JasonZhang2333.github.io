---
layout:     post
title:      "《精通数据科学》笔记"
date:       2019-11-22
author:     "zzc"
header-style: text
mathjax: true
catalog: true
tags:
    - 编程
    - 统计
    - 机器学习
    - 笔记
---

### 前言

以统计分析为基础的统计学派，注重假设和模型解释，但大都是线性模型，太过简单，无法处理复杂的现实数据；以机器学习为基础的人工智能学派，注重模型预测效果，但模型缺乏理论依据、无法解释，难以帮助理解数据，两者都以计算机为基础，统计学、机器学习和计算机科学构成了数据科学。

### 数据科学概述

> The purpose of computing is insight, not numbers.
>                             ——Richard Hamming

###### 挑战

![img](/img/in-post/post-精通数据科学/201911222111.png)

**工程实现的挑战**
- 特征提取
    > 建模项目成功很大程度依赖特征提取，包括数据清洗、数据整合、变量归一化等。
- 矩阵运算
    > 复杂数学模型需要类似随机梯度下降的最优化算法估参，模型越复杂参数越多，需要GPU或TPU。
- 分布式机器学习
    > 在海量数据上使用复杂模型，需要算法在多台机器上并行运行。

**模型搭建的挑战**
- 模型预测效果好
    > 模型的预测效果取决于假设是否被满足，因为模型除去假设部分的其他推导都是严谨的数学演算。数据满足假设，模型效果一定不会差，但具体问题很难找到假设完全满足的模型，所以要通过特征提取等手段，尽量避免违反那些对结果影响很大的假设。
- 模型参数稳定且“正确”
    > 模型除了预测还要对已有数据做分析，比如某个变量对结果的影响，但模型参数的估计值是随机变量，取决于训练模型使用的数据，所以要求参数估计是正确（期望等于真实值）且稳定（方差不能太大）的。
- 模型结果容易解释
    > 模型实际应用会涉及很多非技术人员，要求模型结果直观，容易被解释。

模型搭建有两种截然不同的思路：
- 统计模型
    > 假设数据生成过程，通过模型理解整个过程，模型结果容易解释，参数稳定正确，但模型预测效果不太好。
- 机器学习
    > 假设数据生成过程复杂且未知，建模只是从结构上模仿生成过程，预测效果较好，模型可解释性差，稳定性分析也不多。

所以要将两种模型结合起来，使用机器学习模型对数据建模，然后借鉴统计模型的分析工具分析模型的稳定性和结果解释。

###### 机器学习

传统的编程是人类把积累的经验转换为规则，然后用编程语言表示出这些规则，而机器学习则不需要具体的规则，只需制订学习的步骤，然后输入大量数据，计算机根据数据和步骤自己总结经验升级。

![img](/img/in-post/post-精通数据科学/201911231035.png)

机器学习根据训练数据类型分为两类：
- 监督式学习：训练数据有标注
    1. 分类：标注是离散的
    2. 回归：标注是连续的
- 非监督式学习：训练数据没有标注
    1. 聚类：把“距离”相近的点归为一类
    2. 降维：把高位空间的数据映射到低维空间

###### 统计模型

![img](/img/in-post/post-精通数据科学/201911231239.png)

机器学习非常依赖所用的训练数据，但是有时表面上找到了数据变动的规律，其实只是随机扰动引起的数学巧合，这就是模型幻觉。统计模型会小心处理各种模型，确保摆脱数据中心随机因素的干扰，得到稳定且正确的结论，弥补了机器学习的不足。

###### Python

> If you are immune to boredom, there is literally nothing you cannot accomplish.
>               ——David Foster Wallace

![img](/img/in-post/post-精通数据科学/201911221246.png)

用途 | Python库 | 功能
数据预处理 | `NumPy` | 数组、向量运算
数据预处理 | `SciPy` | 最优化、数值计算
数据预处理 | `pandas` | 结构化数据处理
数据可视化 | `Matplotlib` | 数据图表
标准模型库 | `scikit-learn` | 机器学习
标准模型库 | `Statsmodels` | 统计库，假设检验、置信区间
标准模型库 | `Spark ML` | 分布式机器学习，用于集群
标准模型库 | `PyTorch` | 深度学习

### 数学基础

> If people do not believe that mathematics is simple, it is only because they do not realize how complicated life is.
>                       ——Jonh von Neumann

###### 矩阵

`Python`提供两种表示矩阵的方法，一种是`matrix`类，默认乘法为矩阵乘法，另一种是二维`array`，默认乘法是Hadamard乘法，第三方库`NumPy`还提供专门的函数创建矩阵。

*矩阵创建*
``` python
In [1]: import numpy as np

In [2]: from numpy.linalg import inv

In [3]: A = np.matrix([[1, 2], [3, 4], [5, 6]])

In [4]: B = np.array(range(1, 7)).reshape(3, 2)

In [5]: B * B
Out[5]:
array([[ 1,  4],
       [ 9, 16],
       [25, 36]])

In [6]: np.zeros((3, 2))
Out[6]:
array([[0., 0.],
       [0., 0.],
       [0., 0.]])

In [7]: np.identity(3)
Out[7]:
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]])

In [8]: np.diag([1, 2, 3])
Out[8]:
array([[1, 0, 0],
       [0, 2, 0],
       [0, 0, 3]])

In [9]: m = np.array(range(1, 10)).reshape(3, 3)

In [10]: m[[0, 2]]
Out[10]:
array([[1, 2, 3],
       [7, 8, 9]])

In [11]: m[:, [1, 2]]
Out[11]:
array([[2, 3],
       [5, 6],
       [8, 9]])
```

*矩阵运算*
``` python
In [1]: import numpy as np

In [2]: from numpy.linalg import inv

In [3]: n = np.array(range(1, 5)).reshape(2, 2)

In [4]: np.transpose(n)
Out[4]:
array([[1, 3],
       [2, 4]])

In [5]: n + n
Out[5]:
array([[2, 4],
       [6, 8]])

In [6]: n - n
Out[6]:
array([[0, 0],
       [0, 0]])

In [7]: 3 * n
Out[7]:
array([[ 3,  6],
       [ 9, 12]])

In [8]: n * n
Out[8]:
array([[ 1,  4],
       [ 9, 16]])

In [9]: n.dot(n)
Out[9]:
array([[ 7, 10],
       [15, 22]])

In [10]: inv(n)
Out[10]:
array([[-2. ,  1. ],
       [ 1.5, -0.5]])

In [11]: np.dot(inv(n), n)
Out[11]:
array([[1.0000000e+00, 4.4408921e-16],
       [0.0000000e+00, 1.0000000e+00]])
```

###### 向量空间

向量$A=(a_1,a_2,a_3)$与向量$B=(b_1,b_2,b_3)$的内积定义为：

$$A·B=\|A\|\|B\|cos\theta=a_1b_1+a_2b_2+a_3b_3$$

把行向量看做三维空间的一个点，则$n\times 3$的矩阵$X$可以看做三维空间里点的集合，现在要在空间中找到一条直线，使得矩阵中的点在这条直线的投影之和最大，则直线对应的单位向量满足如下条件：

$$\nu=argmax_{\|w\|=1}\|Xw^T\|^2=argmax_{\|w\|=1}wX^TXw^T$$

要解决这一问题，需要引入特征向量(eigenvector)和特征值(eigenvalue)的概念，满足如下条件的非零行向量$w$被称为$M$的特征向量，对应的$\lambda$被称为特征值：

$$Mw^T=\lambda w^T$$

当$M$是一个三阶对称矩阵，即$M^T=M$时，存在3个相互正交的特征向量，可以组成三维空间的一组基。假设$w_1,w_2,w_3$是对称矩阵$X^TX$的长度为1且相互正交的特征向量，对应特征值为$\lambda_1,\lambda_2,\lambda_3$且$\lambda_1 \ge \lambda_2 \ge \lambda_3$,任何一个长度为1的行向量$w$可以写为$w=aw_1+bw_2+cw_3$，其中$a^2+b^2+c^2=1$，则：

$$wX^TXw^T=\lambda_1a^2+\lambda_2b^2+\lambda_3c^2 \le \lambda_1(a^2+b^2+c^2)=\lambda_1$$

所以$\nu$就是$X^TX$最大特征值对应的特征向量。

###### 概率

贝叶斯定理：

![img](/img/in-post/post-精通数据科学/201911221621.png)

$$P(B|A)=P(A\bigcap B)/P(A)=P(A|B)P(B)/P(A)$$

方差(variance)：

$$Var(X)=E[(X-E[X])^2]=E[X^2]-(E[X])^2$$

协方差(covariance):

$$Cov(X,Y)=E[(X-E[X])(Y-E[Y])]=E[XY]-E[X]E[Y]$$

$$Var(aX+bY)=a^2Var(X)+b^2Var(Y)+2abCov(X,Y)$$

正态分布：

![img](/img/in-post/post-精通数据科学/201911221704.png)

$$f(x)=\frac{1}{\sqrt{2\pi \sigma^2}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}$$

$$X\sim N(\mu,\sigma^2)$$

**为什么正态分布会如此广泛存在？**

中心极限定理：假设随机变量$X_1,X_2,...,X_n$独立同分布且具有有限期望和方差，记为$E[X_i]=m,Var(X_i)=\nu^2$，有如下结论

$$\overline{X}=\frac{1}{n}\sum_{i=1}^n X_n$$

$$T_n=\frac{\overline{X}-m}{\nu/\sqrt{n}}$$

$$\lim_{n \to \infty}T_n \sim N(0,1)$$

上述公式表示在一定条件下，不管随机变量的分布如何，它们的和经过一定的线性变换后会逼近一个标准正态分布，也就是说，一定量的随机效应叠加起来就近似服从正态分布，现实中很多观察到的随机变量实际上正是多个独立同分布的随机变量叠加起来的结果，大致服从一个正态分布。

